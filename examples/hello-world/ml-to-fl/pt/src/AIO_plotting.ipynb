{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from net import FraudNet, AttentionTransformerFraudNet, EnhancedFraudNet  # Import fraud detection model\n",
    "from data import get_dataloaders_fraud  # Import dataset functions\n",
    "from evaluation import evaluate_model  # Import evaluation function\n",
    "from train import train_model, set_all_seeds  # Import training function from train.py\n",
    "import pandas as pd\n",
    "import sys\n",
    "from plot import plot_metrics, plot_confusion_matrices, plot_aucpr\n",
    "import pickle\n",
    "\n",
    "\n",
    "# Load fraud dataset\n",
    "set_all_seeds(42)\n",
    "\n",
    "# Set dataset path\n",
    "DATASET_PATH = \"/home/khoa/Khoa/outsource/na_thesis/examples/hello-world/ml-to-fl/pt/src/data/creditcard.csv\"\n",
    "DEVICE = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "# Training hyperparameters\n",
    "batch_size = 32\n",
    "num_epochs = 15\n",
    "learning_rate = 0.00003\n",
    "\n",
    "df = pd.read_csv(DATASET_PATH)\n",
    "input_size = df.shape[1] - 1\n",
    "print(f\"Detected input size: {input_size}\")\n",
    "\n",
    "train_loader, valid_loader, test_loader, class_weights = get_dataloaders_fraud(\n",
    "    DATASET_PATH, batch_size=batch_size, use_smote=True, plot=True, save_plot_dir='data_plot'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for model in [AttentionTransformerFraudNet(input_size=input_size).to(DEVICE), FraudNet(input_size=input_size).to(DEVICE), EnhancedFraudNet(input_size=input_size).to(DEVICE)]:\n",
    "    # Get model name for saving metrics\n",
    "    model_name = model.__class__.__name__\n",
    "\n",
    "    class_weights = class_weights\n",
    "    pos_weight = torch.tensor([class_weights[1] / class_weights[0]], device=DEVICE)\n",
    "\n",
    "    pos_weight = None\n",
    "\n",
    "    # Loss Function (No weight balancing since using SMOTE)\n",
    "    criterion = nn.BCEWithLogitsLoss(pos_weight=pos_weight)\n",
    "\n",
    "    # Optimizer\n",
    "    optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "    # Call `train.py` instead of writing the training loop here\n",
    "    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
    "        optimizer, mode='max', patience=3, verbose=True\n",
    "    )\n",
    "\n",
    "    train_loss_list, train_metrics_list, valid_metrics_list, test_metrics = train_model(\n",
    "        model, num_epochs, train_loader, valid_loader, test_loader, optimizer,\n",
    "        criterion, DEVICE, scheduler=scheduler, stochastic=False\n",
    "    )\n",
    "\n",
    "    if pos_weight:\n",
    "        save_plot_dir = f'plot_{model_name}_{batch_size}_{num_epochs}_{learning_rate}_pos_weight'\n",
    "    else:\n",
    "        save_plot_dir = f'plot_{model_name}_{batch_size}_{num_epochs}_{learning_rate}'\n",
    "        \n",
    "    os.makedirs(save_plot_dir, exist_ok=True)\n",
    "\n",
    "    # Create metrics directory if it doesn't exist\n",
    "    metrics_dir = f'metrics'\n",
    "    os.makedirs(metrics_dir, exist_ok=True)\n",
    "\n",
    "    # Save training metrics\n",
    "    metrics_data = {\n",
    "        'train_metrics': train_metrics_list,\n",
    "        'valid_metrics': valid_metrics_list,\n",
    "        'test_metrics': test_metrics,\n",
    "        'train_loss': train_loss_list\n",
    "    }\n",
    "    if pos_weight:\n",
    "        metrics_file = os.path.join(metrics_dir, f\"{model_name}_{batch_size}_{num_epochs}_{learning_rate}_pos_weight_metrics.pickle\")\n",
    "    else:\n",
    "        metrics_file = os.path.join(metrics_dir, f\"{model_name}_{batch_size}_{num_epochs}_{learning_rate}_metrics.pickle\")\n",
    "        \n",
    "    with open(metrics_file, 'wb') as f:\n",
    "        pickle.dump(metrics_data, f)\n",
    "\n",
    "    print(f\"Metrics saved to {metrics_file}\")\n",
    "\n",
    "    # Create plots\n",
    "    plot_metrics(train_metrics_list, fig_name=\"Training Metrics\", save_path=f\"{save_plot_dir}/train_metrics.png\")\n",
    "    plot_metrics(valid_metrics_list, fig_name=\"Validation Metrics\", save_path=f\"{save_plot_dir}/valid_metrics.png\")\n",
    "    plot_confusion_matrices(model, test_loader, threshold=0.85, save_path=f\"{save_plot_dir}/confusion_matrix.png\")\n",
    "    plot_aucpr(model, test_loader, device=DEVICE, save_path=f\"{save_plot_dir}/auc_pr.png\")\n",
    "\n",
    "    # Save the trained model\n",
    "    best_model_path = \"best_model.pth\"\n",
    "    model.load_state_dict(torch.load(best_model_path))\n",
    "    print(\"Loaded best model from training phase.\")\n",
    "\n",
    "    # Save the best model explicitly at a clear location for future usage\n",
    "    if pos_weight:\n",
    "        final_model_path = f\"./best_{model_name}_{batch_size}_{num_epochs}_{learning_rate}_pos_weight_model.pth\"\n",
    "    else:\n",
    "        final_model_path = f\"./best_{model_name}_{batch_size}_{num_epochs}_{learning_rate}_model.pth\"\n",
    "        \n",
    "    torch.save(model.state_dict(), final_model_path)\n",
    "    print(f\"Final best model saved explicitly at {final_model_path}\")\n",
    "\n",
    "    # Evaluate model\n",
    "    print(\"Evaluating Model on Test Set...\")\n",
    "    evaluate_model(model, test_loader, DEVICE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AIO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from net import FraudNet, AttentionTransformerFraudNet, EnhancedFraudNet  # Import fraud detection model\n",
    "from data import get_dataloaders_fraud  # Import dataset functions\n",
    "from evaluation import evaluate_model  # Import evaluation function\n",
    "from train import train_model, set_all_seeds  # Import training function from train.py\n",
    "import pandas as pd\n",
    "import sys\n",
    "from plot import plot_metrics, plot_confusion_matrices, plot_aucpr\n",
    "import pickle\n",
    "\n",
    "\n",
    "# Load fraud dataset\n",
    "set_all_seeds(42)\n",
    "\n",
    "# Set dataset path\n",
    "DATASET_PATH = \"/home/khoa/Khoa/outsource/na_thesis/examples/hello-world/ml-to-fl/pt/src/data/creditcard.csv\"\n",
    "DEVICE = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "# Training hyperparameters\n",
    "batch_size = 32\n",
    "\n",
    "for batch_size in [96, 128, 256]:\n",
    "    num_epochs = 15\n",
    "    learning_rate = 0.00003\n",
    "\n",
    "    df = pd.read_csv(DATASET_PATH)\n",
    "    input_size = df.shape[1] - 1\n",
    "    print(f\"Detected input size: {input_size}\")\n",
    "\n",
    "    train_loader, valid_loader, test_loader, class_weights = get_dataloaders_fraud(\n",
    "        DATASET_PATH, batch_size=batch_size, use_smote=True, plot=True, save_plot_dir='data_plot'\n",
    "    )\n",
    "\n",
    "    for model in [AttentionTransformerFraudNet(input_size=input_size).to(DEVICE), FraudNet(input_size=input_size).to(DEVICE), EnhancedFraudNet(input_size=input_size).to(DEVICE)]:\n",
    "        for pos_weight in [torch.tensor([class_weights[1] / class_weights[0]], device=DEVICE), None]:\n",
    "            # Get model name for saving metrics\n",
    "            model_name = model.__class__.__name__\n",
    "\n",
    "            # Loss Function (No weight balancing since using SMOTE)\n",
    "            criterion = nn.BCEWithLogitsLoss(pos_weight=pos_weight)\n",
    "\n",
    "            # Optimizer\n",
    "            optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "            # Call `train.py` instead of writing the training loop here\n",
    "            scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
    "                optimizer, mode='max', patience=3, verbose=True\n",
    "            )\n",
    "\n",
    "            train_loss_list, train_metrics_list, valid_metrics_list, test_metrics = train_model(\n",
    "                model, num_epochs, train_loader, valid_loader, test_loader, optimizer,\n",
    "                criterion, DEVICE, scheduler=scheduler, stochastic=False\n",
    "            )\n",
    "\n",
    "            if pos_weight:\n",
    "                save_plot_dir = f'plot_{model_name}_{batch_size}_{num_epochs}_{learning_rate}_pos_weight'\n",
    "            else:\n",
    "                save_plot_dir = f'plot_{model_name}_{batch_size}_{num_epochs}_{learning_rate}'\n",
    "                \n",
    "            os.makedirs(save_plot_dir, exist_ok=True)\n",
    "\n",
    "            # Create metrics directory if it doesn't exist\n",
    "            metrics_dir = f'metrics'\n",
    "            os.makedirs(metrics_dir, exist_ok=True)\n",
    "\n",
    "            # Save training metrics\n",
    "            metrics_data = {\n",
    "                'train_metrics': train_metrics_list,\n",
    "                'valid_metrics': valid_metrics_list,\n",
    "                'test_metrics': test_metrics,\n",
    "                'train_loss': train_loss_list\n",
    "            }\n",
    "            if pos_weight:\n",
    "                metrics_file = os.path.join(metrics_dir, f\"{model_name}_{batch_size}_{num_epochs}_{learning_rate}_pos_weight_metrics.pickle\")\n",
    "            else:\n",
    "                metrics_file = os.path.join(metrics_dir, f\"{model_name}_{batch_size}_{num_epochs}_{learning_rate}_metrics.pickle\")\n",
    "                \n",
    "            with open(metrics_file, 'wb') as f:\n",
    "                pickle.dump(metrics_data, f)\n",
    "\n",
    "            print(f\"Metrics saved to {metrics_file}\")\n",
    "\n",
    "            # Create plots\n",
    "            plot_metrics(train_metrics_list, fig_name=\"Training Metrics\", save_path=f\"{save_plot_dir}/train_metrics.png\")\n",
    "            plot_metrics(valid_metrics_list, fig_name=\"Validation Metrics\", save_path=f\"{save_plot_dir}/valid_metrics.png\")\n",
    "            plot_confusion_matrices(model, test_loader, threshold=0.85, save_path=f\"{save_plot_dir}/confusion_matrix.png\")\n",
    "            plot_aucpr(model, test_loader, device=DEVICE, save_path=f\"{save_plot_dir}/auc_pr.png\")\n",
    "\n",
    "            # Save the trained model\n",
    "            best_model_path = \"best_model.pth\"\n",
    "            model.load_state_dict(torch.load(best_model_path))\n",
    "            print(\"Loaded best model from training phase.\")\n",
    "\n",
    "            # Save the best model explicitly at a clear location for future usage\n",
    "            if pos_weight:\n",
    "                final_model_path = f\"./best_{model_name}_{batch_size}_{num_epochs}_{learning_rate}_pos_weight_model.pth\"\n",
    "            else:\n",
    "                final_model_path = f\"./best_{model_name}_{batch_size}_{num_epochs}_{learning_rate}_model.pth\"\n",
    "                \n",
    "            torch.save(model.state_dict(), final_model_path)\n",
    "            print(f\"Final best model saved explicitly at {final_model_path}\")\n",
    "\n",
    "            # Evaluate model\n",
    "            print(\"Evaluating Model on Test Set...\")\n",
    "            evaluate_model(model, test_loader, DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "DIR = '/home/khoa/Khoa/outsource/na_thesis/examples/hello-world/ml-to-fl/pt/src/metrics'\n",
    "metrics = os.listdir(DIR)\n",
    "metrics.sort()\n",
    "\n",
    "for metric in metrics:\n",
    "    if metric.startswith('Attention') and 'pos_weight' not in metric:\n",
    "        with open(os.path.join(DIR, metric), 'rb') as f:\n",
    "            data = pickle.load(f)\n",
    "        \n",
    "        print(data.keys())\n",
    "        print(data['train_metrics'][1].keys())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Note\n",
    "## train_loss for comparison between model (same batch size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Compare Model with itself in diff batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from scipy.signal import savgol_filter\n",
    "import re\n",
    "\n",
    "def sort_by_model_and_size(filename):\n",
    "    # Extract model name (everything before the first underscore)\n",
    "    model_match = re.match(r'([^_]+)_', filename)\n",
    "    model_name = model_match.group(1) if model_match else \"\"\n",
    "    \n",
    "    # Extract size (first number after the model name)\n",
    "    size_match = re.search(r'_(\\d+)_', filename)\n",
    "    size = int(size_match.group(1)) if size_match else 0\n",
    "    \n",
    "    # Return tuple for sorting (first by model, then by size)\n",
    "    return (model_name, size)\n",
    "\n",
    "# Directory where metrics are stored\n",
    "DIR = '/home/khoa/Khoa/outsource/na_thesis/examples/hello-world/ml-to-fl/pt/src/metrics'\n",
    "METRIC_NAMES = ['train_metrics', 'valid_metrics']\n",
    "METRIC_PERSIONS =  ['accuracy', 'precision', 'recall', 'f1_score', 'auc_roc', 'auc_pr', 'loss']\n",
    "\n",
    "models = [FraudNet(), EnhancedFraudNet(), AttentionTransformerFraudNet()]\n",
    "for model in models:\n",
    "    for metric_name in METRIC_NAMES:\n",
    "        for metric_percision in METRIC_PERSIONS:\n",
    "            model_name = model.__class__.__name__\n",
    "            \n",
    "            metrics = os.listdir(DIR)\n",
    "            metrics.sort()\n",
    "            \n",
    "            \n",
    "\n",
    "            # Set up the plot\n",
    "            plt.figure(figsize=(12, 6))\n",
    "            plt.title(f'{metric_name}_{metric_percision} in {model_name}', fontsize=14)\n",
    "            plt.xlabel('Step', fontsize=12)\n",
    "            plt.ylabel('Metric Value', fontsize=12)\n",
    "\n",
    "            # Colors for different models with better contrast\n",
    "            colors = ['#1f77b4', '#ff7f0e', '#2ca02c', '#d62728', '#9467bd', '#8c564b', '#e377c2', '#7f7f7f', '#bcbd22', '#17becf']\n",
    "            markers = ['o', 's', '^', 'D', 'x', '*']\n",
    "\n",
    "            # Load and plot each metric file\n",
    "            attention_models = []\n",
    "            for i, metric in enumerate(sorted(metrics, key=sort_by_model_and_size)):\n",
    "                if metric.startswith(model_name):\n",
    "                    attention_models.append(metric)\n",
    "                    with open(os.path.join(DIR, metric), 'rb') as f:\n",
    "                        data = pickle.load(f)\n",
    "                            \n",
    "                    # Extract x and y values for plotting\n",
    "                    x_values = []\n",
    "                    y_values = []\n",
    "                    \n",
    "                    for j, point in enumerate(data[metric_name]):\n",
    "                        x_values.append(j)\n",
    "                        y_values.append(point[metric_percision])\n",
    "                    \n",
    "                    # Plot each point individually\n",
    "                    for j in range(len(x_values)):\n",
    "                        plt.plot(x_values[j], y_values[j], \n",
    "                                marker=markers[i % len(markers)], \n",
    "                                color=colors[i % len(colors)],\n",
    "                                markersize=5)\n",
    "                    \n",
    "                    # Connect points with a line\n",
    "                    plt.plot(x_values, y_values, \n",
    "                            color=colors[i % len(colors)], \n",
    "                            linewidth=1.5, \n",
    "                            alpha=0.7,\n",
    "                            label=f'{metric}')\n",
    "\n",
    "            # Add legend with better placement\n",
    "            plt.legend(loc='best', fontsize=10)\n",
    "\n",
    "            # Add grid for better readability but make it subtle\n",
    "            plt.grid(True, linestyle='--', alpha=0.3)\n",
    "\n",
    "            # Improve appearance\n",
    "            plt.tight_layout()\n",
    "\n",
    "            # Save the figure\n",
    "            plt.savefig(f'{model_name}_{metric_name}_{metric_percision}_pos_weight_points.png', dpi=300, bbox_inches='tight')\n",
    "\n",
    "            # Show the plot\n",
    "            plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Same Batch Size Diff Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All plots created successfully!\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pickle\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import re\n",
    "from matplotlib.lines import Line2D\n",
    "\n",
    "# Directory where metrics are stored\n",
    "DIR = '/home/khoa/Khoa/outsource/na_thesis/examples/hello-world/ml-to-fl/pt/src/metrics'\n",
    "METRIC_NAMES = ['train_metrics', 'valid_metrics']\n",
    "METRIC_PERSIONS = ['accuracy', 'precision', 'recall', 'f1_score', 'auc_roc', 'auc_pr', 'loss']\n",
    "\n",
    "# Define batch sizes and models to compare\n",
    "BATCH_SIZE = \"256\"  # Set this to the batch size you want to compare\n",
    "MODEL_NAMES = [\"FraudNet\", \"EnhancedFraudNet\", \"AttentionTransformerFraudNet\"]\n",
    "\n",
    "# Function to check if a file matches our criteria\n",
    "def matches_criteria(filename, model_name, batch_size, use_pos_weight):\n",
    "    # Check if file starts with the model name\n",
    "    if not filename.startswith(model_name):\n",
    "        return False\n",
    "    \n",
    "    # Check if file contains the specified batch size\n",
    "    batch_match = re.search(r'_(\\d+)_', filename)\n",
    "    if not (batch_match and batch_match.group(1) == batch_size):\n",
    "        return False\n",
    "    \n",
    "    # Check if file has \"pos_weight\" according to the preference\n",
    "    has_pos_weight = \"pos_weight\" in filename\n",
    "    if has_pos_weight != use_pos_weight:\n",
    "        return False\n",
    "    \n",
    "    return True\n",
    "\n",
    "# Create plots for each metric\n",
    "for metric_name in METRIC_NAMES:\n",
    "    for metric_precision in METRIC_PERSIONS:\n",
    "        # Set up the plot\n",
    "        plt.figure(figsize=(14, 8))\n",
    "        plt.title(f'Comparison of Models: {metric_name}_{metric_precision} (Batch Size {BATCH_SIZE})', \n",
    "                 fontsize=14)\n",
    "        plt.xlabel('Step', fontsize=12)\n",
    "        plt.ylabel(f'{metric_precision.replace(\"_\", \" \").title()}', fontsize=12)\n",
    "\n",
    "        # Colors for different models with better contrast\n",
    "        colors = ['#1f77b4', '#ff7f0e', '#2ca02c', '#d62728', '#9467bd', '#8c564b']\n",
    "        \n",
    "        # Different line styles for pos_weight vs regular\n",
    "        line_styles = ['-', '--']  # Solid for regular, dashed for pos_weight\n",
    "        markers = ['o', 's', '^', 'D', 'x', '*']\n",
    "\n",
    "        # Get all metrics files\n",
    "        metrics_files = os.listdir(DIR)\n",
    "        \n",
    "        # To store data for custom legend\n",
    "        legend_elements = []\n",
    "        \n",
    "        # For each model, find and plot both the pos_weight and regular files\n",
    "        for i, model_name in enumerate(MODEL_NAMES):\n",
    "            color = colors[i % len(colors)]\n",
    "            marker = markers[i % len(markers)]\n",
    "            \n",
    "            for weight_type in [False, True]:  # False = regular, True = pos_weight\n",
    "                model_found = False\n",
    "                for metric_file in metrics_files:\n",
    "                    if matches_criteria(metric_file, model_name, BATCH_SIZE, weight_type):\n",
    "                        with open(os.path.join(DIR, metric_file), 'rb') as f:\n",
    "                            try:\n",
    "                                data = pickle.load(f)                                \n",
    "                                # Extract x and y values for plotting\n",
    "                                x_values = []\n",
    "                                y_values = []\n",
    "                                \n",
    "                                for j, point in enumerate(data[metric_name]):\n",
    "                                    # Check if the metric precision exists in the point data\n",
    "                                    if isinstance(point, dict) and metric_precision in point:\n",
    "                                        x_values.append(j)\n",
    "                                        y_values.append(point[metric_precision])\n",
    "                                \n",
    "                                if len(x_values) > 0:\n",
    "                                    # Choose line style by weight type\n",
    "                                    line_style = line_styles[1 if weight_type else 0]\n",
    "                                    \n",
    "                                    # Create appropriate label\n",
    "                                    label = f\"{model_name} {'(pos_weight)' if weight_type else '(regular)'}\"\n",
    "                                    \n",
    "                                    # Plot the line\n",
    "                                    plt.plot(x_values, y_values,\n",
    "                                            color=color,\n",
    "                                            linestyle=line_style,\n",
    "                                            linewidth=2,\n",
    "                                            alpha=0.7)\n",
    "                                    \n",
    "                                    # Plot the points\n",
    "                                    plt.scatter(x_values, y_values,\n",
    "                                              marker=marker,\n",
    "                                              color=color,\n",
    "                                              s=30,\n",
    "                                              alpha=0.8 if weight_type else 0.6)\n",
    "                                    \n",
    "                                    # Create a legend element for this line style\n",
    "                                    legend_elements.append(\n",
    "                                        Line2D([0], [0], color=color, marker=marker, linestyle=line_style,\n",
    "                                              markersize=8, label=label)\n",
    "                                    )\n",
    "                                    \n",
    "                                    model_found = True\n",
    "                                else:\n",
    "                                    print(f\"No data points found for {metric_precision} in {metric_file}\")\n",
    "                                    \n",
    "                            except Exception as e:\n",
    "                                print(f\"Error processing {metric_file}: {e}\")\n",
    "                        \n",
    "                        if model_found:\n",
    "                            # We only need one file per model/weight combo, so break after finding the first match\n",
    "                            break\n",
    "                \n",
    "                if not model_found:\n",
    "                    weight_label = \"pos_weight\" if weight_type else \"regular\"\n",
    "        \n",
    "        # Add a custom legend that correctly shows line styles and markers\n",
    "        if legend_elements:\n",
    "            plt.legend(handles=legend_elements, loc='best', fontsize=10, ncol=2)\n",
    "        else:\n",
    "            plt.close()\n",
    "            continue\n",
    "        \n",
    "        # Add grid for better readability but make it subtle\n",
    "        plt.grid(True, linestyle='--', alpha=0.3)\n",
    "        \n",
    "        # Set background color to light gray for better contrast\n",
    "        plt.gca().set_facecolor('#f8f8f8')\n",
    "        \n",
    "        # Improve appearance\n",
    "        plt.tight_layout()\n",
    "        \n",
    "        # Save the figure\n",
    "        plt.savefig(f'comparison_{metric_name}_{metric_precision}_batch{BATCH_SIZE}_combined.png', \n",
    "                   dpi=300, bbox_inches='tight')\n",
    "                \n",
    "        # Close the figure to free memory\n",
    "        plt.close()\n",
    "\n",
    "print(\"All plots created successfully!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "khoa",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
